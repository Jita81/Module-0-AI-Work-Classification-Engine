# Automated Agile Microservice Factory - Cursor Rules

## üéØ Project Overview
This project builds a JSON-to-microservice pipeline using automated agile methodology. We're creating a self-improving system that generates production-ready microservices from JSON specifications while using the same process to enhance itself.

## üìÅ Repository Structure Understanding

### Core Framework (`src/core/`)
- `generators/`: Module generation engine with pattern templates
- `patterns/`: Infrastructure patterns (web_api, background_worker, etc.)
- `ai_integration/`: Cursor CLI automation and AI completion workflows
- `quality_gates/`: Code review and testing automation

### JSON Pipeline (`src/json_pipeline/`)
- `spec_parser.py`: Parse JSON microservice specifications
- `microservice_factory.py`: Generate microservices from JSON specs
- `tdd_scaffolder.py`: Create TDD tests from JSON requirements
- `github_orchestrator.py`: Coordinate GitHub Actions workflows

### Automated Agile (`src/automated_agile/`)
- `sprint_manager.py`: Automate sprint lifecycle (planning ‚Üí execution ‚Üí review ‚Üí retro)
- `backlog_generator.py`: Generate sprint backlogs from JSON specifications
- `retrospective_analyzer.py`: Extract learnings and improvement actions
- `success_classifier.py`: Determine automation vs human escalation

### Sprint Structure (`sprints/sprint-{number}/`)
- `planning/`: Sprint goals, backlogs, capacity plans, definition of done
- `execution/`: Daily progress, impediments, burndown metrics
- `review/`: Demo artifacts, stakeholder feedback, acceptance verification
- `retrospective/`: What worked, improvements, action items, metrics analysis

### Generated Microservices (`microservices/{name}/`)
- `sprint-artifacts/`: Complete sprint documentation for the microservice
- `src/`: Generated microservice code following framework patterns
- `tests/`: TDD tests derived from JSON specifications
- `.github/workflows/`: Microservice-specific automation pipelines

## üöÄ Development Approach

### 1. Automated Agile Methodology
- **Each sprint** (1 week) has automated planning, execution tracking, and retrospectives
- **JSON specifications** drive both microservice generation and framework improvements
- **Recursive self-improvement**: Use the pipeline to enhance the pipeline itself
- **Success metrics** determine automation vs human escalation

### 2. Self-Improving System
- Framework components are generated using JSON specs starting Sprint 3
- Success rates classify which patterns work best with automation
- Module library evolves based on usage feedback and performance metrics
- Automated retrospectives feed improvements back into the system

### 3. Quality-First Automation
- Every generated microservice passes through automated code review
- TDD tests are generated from JSON specs and must pass before completion
- Up to 3 automated fix cycles using context-aware improvements
- Zero-regression policy: new microservices don't break existing functionality

## üíª Coding Standards

### File Organization
- Follow the target repository structure strictly
- Place sprint artifacts in proper directories with clear naming
- Use consistent naming: `snake_case` for Python, `kebab-case` for files/directories
- Include comprehensive docstrings for all modules and functions

### JSON Specification Format
```json
{
  "microservice_type": "descriptive-name",
  "domain": "business-domain",
  "tdd_tests": [
    {
      "test_name": "specific_test_case",
      "description": "Clear test description",
      "expected_behavior": "Specific expected outcome"
    }
  ],
  "requirements": {
    "performance": "< Xms response time",
    "security": "Security requirements",
    "integration": ["external", "services"]
  }
}
```

### Code Generation Rules
- **Always generate TDD tests first** based on JSON specifications
- **Follow existing framework patterns** from `src/core/patterns/`
- **Include proper error handling** and logging in all generated code
- **Add comprehensive documentation** including AI completion guides
- **Ensure backward compatibility** with existing framework components

### Testing Requirements
- **Unit tests** for all framework components (>90% coverage)
- **Integration tests** for JSON pipeline end-to-end functionality
- **Generated microservice validation** to ensure quality output
- **Automated regression testing** to prevent framework degradation

## üîÑ Sprint Workflow Integration

### Sprint Planning (Automated)
When working on sprint planning:
- Generate sprint goals from JSON specifications
- Create detailed backlogs with acceptance criteria
- Establish definition of done with measurable success criteria
- Plan capacity based on automation vs human effort estimation

### Sprint Execution
During development:
- Update daily progress in `sprints/sprint-{N}/execution/daily-progress/`
- Track impediments and resolution efforts
- Maintain automated burndown metrics
- Follow TDD approach: tests first, then implementation

### Sprint Review & Retrospective
At sprint completion:
- Generate demo artifacts showing working functionality
- Document stakeholder feedback and acceptance verification
- Analyze what worked well and areas for improvement
- Create actionable items for next sprint improvement

## üéØ AI Assistance Guidelines

### Context Awareness
- **Always consider the automated agile context** when generating code
- **Reference existing framework patterns** rather than creating new ones
- **Maintain consistency** with established naming and structure conventions
- **Think recursively**: how can this component improve itself?

### JSON-Driven Development
- **Parse JSON specifications carefully** to understand requirements
- **Generate appropriate TDD tests** that validate JSON requirements
- **Create modular, reusable components** that can be shared across microservices
- **Include metadata** for success classification and improvement tracking

### Quality Focus
- **Write production-ready code** that passes automated quality gates
- **Include comprehensive error handling** and edge case consideration
- **Add detailed logging** for debugging and improvement analysis
- **Optimize for maintainability** since the system will self-improve

### Self-Improvement Integration
- **Consider how each component can be auto-generated** from JSON specs
- **Add metadata for success tracking** and automated classification
- **Design for evolution**: components should be easily updatable
- **Include learning hooks** for automated retrospective analysis

## üìä Success Metrics Integration

### Code Quality Metrics
- Automated code review must find < 3 high-severity issues
- Test coverage must be > 90% for all generated components
- Performance requirements from JSON specs must be met
- Security scans must pass with zero critical vulnerabilities

### Automation Success Metrics
- JSON specs should generate working microservices > 95% of the time
- Automated fix cycles should resolve issues within 3 iterations
- Sprint goals should be achieved through automation > 80% of the time
- Framework self-improvement should reduce manual effort each sprint

### Business Impact Metrics
- Time from JSON spec to deployed microservice < 30 minutes
- Cost reduction compared to manual development > 90%
- Module reuse rate should increase each sprint
- Customer satisfaction with generated microservices > 4.5/5

## üîß Development Workflow

1. **Understand the sprint context**: Read current sprint planning documents
2. **Follow JSON specifications**: Implement exactly what's specified in JSON
3. **Generate tests first**: TDD approach with tests derived from JSON specs
4. **Use existing patterns**: Leverage framework patterns rather than creating new ones
5. **Document thoroughly**: Include sprint artifacts and learning documentation
6. **Test automatically**: Ensure all quality gates pass before completion
7. **Deploy and review regularly**: Use automated pipeline for continuous deployment
8. **Integrate AI feedback**: Act on AI Code Review findings immediately
9. **Track sprint progress**: Update automated agile metrics daily
10. **Plan for evolution**: Consider how this component can self-improve

## ü§ñ Integrated Quality System (AI Review + Framework Validation)

### Unified Quality Workflow
- **Every commit**: Automatically triggers AI code review + framework validation via GitHub Actions
- **Dual quality gates**: AI Review (75+) + Framework Compliance (90+) = Overall (80+)
- **365% detection rate**: Leverages proven CODEREVIEW tool with zero false positives
- **Framework validation**: CODETEST ensures Standardized Modules compliance
- **Immediate feedback**: Combined analysis results available within minutes

### Quality Gate Thresholds
- **AI Code Review**: ‚â•75/100 (60% weight) - Security, performance, quality analysis
- **Framework Validation**: ‚â•90/100 (40% weight) - Structure, patterns, compliance
- **Overall Quality**: ‚â•80/100 (combined score) - Must pass both gates
- **Module Type Aware**: Different validation for CORE, INTEGRATION, SUPPORTING, TECHNICAL

### Acting on Integrated Feedback
- **Critical issues (AI)**: Must be addressed before merge - security vulnerabilities, performance issues
- **Framework violations**: Must fix structure/pattern compliance before merge
- **High severity**: Address within same sprint, update framework patterns if systemic
- **Medium/Low**: Include in sprint backlog, analyze trends for framework improvements
- **Pattern optimization**: Use feedback to enhance framework templates and generators
- **Documentation**: Update AI completion guides and framework standards based on insights

### Feedback Integration Loop
- **Pattern Recognition**: Identify recurring issues across modules
- **Framework Evolution**: Update generators and templates based on common feedback
- **Threshold Optimization**: Adjust quality gates based on team performance and business needs
- **Training Data**: Use quality trends for recursive self-improvement optimization

## üöÄ Continuous Deployment Strategy

### Deployment Frequency
- **Every main branch push**: Automatic deployment pipeline execution
- **Quality gates passed**: Deploy only after AI review and automated tests pass
- **Sprint artifacts**: Automatic generation of deployment metrics and progress tracking
- **Version tracking**: Automatic framework evolution metadata collection

### Deployment Artifacts Generated
- **Integrated quality reports**: AI review + framework validation combined analysis
- **Quality trends**: Historical data showing improvement patterns and areas of concern
- **Framework compliance metrics**: Pattern adherence, structure validation, best practice adoption
- **Sprint progress**: Velocity tracking enhanced with quality metrics and improvement indicators
- **Retrospective data**: Quality feedback integration for sprint improvement analysis
- **Pattern optimization suggestions**: Data-driven recommendations for framework enhancements

## üé® UI/UX Considerations

### Generated Microservice UX
- APIs should be intuitive and follow RESTful principles
- Error messages should be clear and actionable
- Documentation should be auto-generated and comprehensive
- Health checks and monitoring should be built-in

### Developer Experience
- JSON specifications should be easy to write and validate
- Generated code should be readable and well-documented
- Debugging should be straightforward with good logging
- The automation should be transparent and trustworthy

---

**Remember**: We're building a system that improves itself. Every component should be designed with automated generation, testing, and evolution in mind. The goal is recursive self-improvement through automated agile methodology.
